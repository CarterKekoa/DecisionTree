{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Part 2: Datasets\n",
    "## Step 2: ðŸš¢ Titanic Classification ðŸš¢\n",
    "Create a decision tree classifier for the titanic dataset. Since each attribute is categorical, you do not need to perform any discretization, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# uncomment once you paste your mypytable.py into mysklearn package\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "source": [
    "* Test your classifier using stratified k-fold cross-validation (with k = 10), and compare your results to those from PA5\n",
    "* Create a confusion matrix for the result and format your results as per PA4 and PA5\n",
    "    * compared to the results of PA5 they are fairly similar. However the Decision tree seemed to be more acurate for 1 yet less so for 2. There also seems to be a loss of 2 data points which is interesting. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===========================================\nDecision Tree Confusion Matrix\n===========================================\n==========  =====  ====  =======  =================\nSurvived      yes    no    Total    Recognition (%)\n==========  =====  ====  =======  =================\n1            1470    20     1490              98.66\n2             441   270      711              37.97\nTotal        1911   290     2201              79.05\n==========  =====  ====  =======  =================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "importlib.reload(myutils)\n",
    "\n",
    "# Get the file data\n",
    "fname = os.path.join(\"input_data\", \"titanic.txt\")\n",
    "titanic_data = MyPyTable().load_from_file(fname)\n",
    "titanic_data.remove_rows_with_missing_values() # prep the data by removing any missing values\n",
    "\n",
    "# Grab the class, age, sex and store in a list\n",
    "titatic_class = titanic_data.get_column('class')\n",
    "titatic_age = titanic_data.get_column('age')\n",
    "titatic_sex = titanic_data.get_column('sex')\n",
    "\n",
    "# split the data\n",
    "X_train = [[titatic_class[i],titatic_age[i],titatic_sex[i]] for i in range(len(titatic_class))]\n",
    "y_train = titanic_data.get_column('survived')\n",
    "\n",
    "# Fit to the Naive Bayes Classifier\n",
    "mdtc = MyDecisionTreeClassifier()\n",
    "mdtc.fit(X_train, y_train) # fit the data using Naive Bayes\n",
    "\n",
    "# fold the column data\n",
    "strattrain_folds, strattest_folds = myevaluation.stratified_kfold_cross_validation(X_train, y_train, 10)\n",
    "X_train_strat, y_train_strat, X_test_strat, y_test_strat = myutils.get_from_folds(X_train, y_train, strattrain_folds, strattest_folds)\n",
    "\n",
    "# make a prediction using Naive Bayes\n",
    "predicted_bayes = mdtc.predict(X_test_strat)\n",
    "\n",
    "print(\"===========================================\")\n",
    "print(\"Decision Tree Confusion Matrix\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# create the confusion matrix\n",
    "matrix = myevaluation.confusion_matrix(y_test_strat, predicted_bayes, ['yes','no'])\n",
    "\n",
    "# print the data\n",
    "table_header = ['Survived', 'yes', 'no', 'Total', 'Recognition (%)']\n",
    "myutils.titanic_stats(matrix)\n",
    "myutils.print_tabulate(matrix, table_header)"
   ]
  },
  {
   "source": [
    "* Print out the rules inferred from your decision tree classifiers when run over the entire dataset (as opposed to the cross validation trees)  \n",
    "  * Based on the rules, determine ways your trees can/should be pruned. Note you do not need to write code to perform pruning, just explain how they can be pruned and give the resulting \"pruned\" rules  \n",
    "  * Cost complexity is a function of:\n",
    "Number of leaves in the tree, and\n",
    "Error-rate of the tree (percent of instances misclassified)\n",
    "We want to minimize both the number of leaves (rules) and error rate  \n",
    "  * Works bottom up:\n",
    "At each attribute node, compute cost complexity\n",
    "Compare this to pruned cost complexity\n",
    "If pruning lowers cost complexity, then prune\n",
    "Otherwise keep un-pruned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IF att2 == female AND att0 == crew AND att1 == adult THEN class = yes\nIF att2 == female AND att0 == crew AND att1 == child THEN class = no\nIF att2 == female AND att0 == first AND att1 == adult THEN class = yes\nIF att2 == female AND att0 == first AND att1 == child THEN class = no\nIF att2 == female AND att0 == second AND att1 == adult THEN class = yes\nIF att2 == female AND att0 == second AND att1 == child THEN class = yes\nIF att2 == female AND att0 == third THEN class = yes\nIF att2 == male AND att0 == crew AND att1 == adult THEN class = no\nIF att2 == male AND att0 == crew AND att1 == child THEN class = no\nIF att2 == male AND att0 == first AND att1 == adult THEN class = no\nIF att2 == male AND att0 == first AND att1 == child THEN class = no\nIF att2 == male AND att0 == second AND att1 == adult THEN class = yes\nIF att2 == male AND att0 == second AND att1 == child THEN class = yes\nIF att2 == male AND att0 == third THEN class = no\n"
     ]
    }
   ],
   "source": [
    "mdtc.print_decision_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}